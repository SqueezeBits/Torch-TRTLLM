# How to Debug Engine Files
## Dumping Debug Artifacts while Building Engine
### Using Ditto
Set the environment variable `DEBUG_ARTIFACTS_DIR` to dump intermediate build artifacts, such as graph module and TensorRT network definition, as files.
```
DEBUG_ARTIFACTS_DIR=./artifacts/ditto-build ditto build meta-llama/Llama-2-7b-chat-hf --output-dir ./engines/meta-llama/Llama-2-7b-chat-hf
```

### Using trtllm-build
Use the [scripts/trtllm-build](scripts/trtllm-build) file instead.
* The only difference is the line `import ditto.patches` added in the original script.

Likewise, set the environment variable `DEBUG_ARTIFACTS_DIR` and **add the flag `--visualize_network --profiling_verbosity detailed`** to dump intermediate build artifacts as files.
```
DEBUG_ARTIFACTS_DIR=./artifacts/trtllm-build /workspace/ditto/scripts/trtllm-build --checkpoint_dir meta-llama/Llama-2-7b-chat-hf-ckpt --output_dir ./engines/Llama-2-7b-chat-hf-trtllm --visualize_network --profiling_verbosity detailed
```
The checkpoints in the directory `meta-llama/Llama-2-7b-chat-hf-ckpt` are generated by one of the native TRT-LLM's `convert_checkpoint.py` scripts, for example,
```
python /workspace/tensorrt_llm/examples/llama/convert_checkpoint.py --model_dir meta-llama/Llama-2-7b-chat-hf --dtype float16 --output_dir meta-llama/Llama-2-7b-chat-hf-ckpt
```

### Note
If you want to create `plugin.txt` file, you need to manually patch the function `gpt_attention_plugin` in the file `/workspace/tensorrt_llm/tensorrt_llm/functional.py` as written in [ditto/patches.py](src/ditto/patches.py).

Just copy and paste the sections wrapped by the following comments at the right place in the code:
```
# ============================ patch start ============================
<the contents to copy&paste>
# ============================ patch end ============================
```


## Adding Extra Output Tensors into Engine
### Using Ditto
* Use the flag `--add-output`, possibly multiple times, to specify the nodes to be added to the outputs.
* See the debug artifacts `graph_module.py` and `graph.txt` to see the node names.

For example, the following command will append the TRT tensors corresponding to the outputs of the FX nodes `mm_default` and `gpt_attention_plugin` to the TensorRT network outputs.
```
ditto build meta-llama/Llama-2-7b-chat-hf --output-dir ./engines/meta-llama/Llama-2-7b-chat-hf-more-outputs --add-output mm_default --add-output gpt_attention_plugin
```

### Using trtllm-build
* Specify the environment variable `TRTLLM_ADD_OUTPUT` in JSON format
* The JSON contents must be a dictionary mapping the TRT layer names to an alias of your choice. It'll be helpful for the comparison later to choose the alias as the corresponding name of the FX node producing it.
* See the debug artifacts `trt_network_def.py` for names of the TRT tensors. The name of the TRT layer that produces each tensor is written in the comment above it.

For example, the following command will append the TRT tensors corresponding to the outputs of the FX nodes `mm_default` and `gpt_attention_plugin` to the TensorRT network outputs in Llama2-7B.
```
TRTLLM_ADD_OUTPUT='{"LLaMAForCausalLM/transformer/layers/0/attention/qkv/multiply_collect_L272/multiply_and_lora_L246/matmul_L1048/cast_L855/CAST_0": "mm_default", "LLaMAForCausalLM/transformer/layers/0/attention/wrapper_L561/gpt_attention_L5154/PLUGIN_V2_GPTAttention_0": "gpt_attention_plugin"}' /workspace/ditto/scripts/trtllm-build --checkpoint_dir meta-llama/Llama-2-7b-chat-hf-ckpt --output_dir ./engines/Llama-2-7b-chat-hf-trtllm-more-outputs
```

## Dumping Stepwise Inputs and Outputs while Running Inference
Use the [scripts/run.py](scripts/run.py) file instead.
* The only difference is the line `import ditto.patches` added in the original script.

Likewise, set the environment variable `DEBUG_ARTIFACTS_DIR` and **add the flags `--use_py_session --debug_mode`** to dump stepwise input and output tensors as files.

For example:
```
DEBUG_ARTIFACTS_DIR=./artifacts/ditto-run \
python /workspace/ditto/scripts/run.py \
    --engine_dir ./engines/meta-llama/Llama-2-7b-chat-hf-more-outputs \
    --tokenizer_dir meta-llama/Llama-2-7b-chat-hf \
    --max_output_len 10 \
    --input_text "Hey, are you conscious?" \
    --use_py_session \
    --debug_mode

DEBUG_ARTIFACTS_DIR=./artifacts/trtllm-run \
python /workspace/ditto/scripts/run.py \
    --engine_dir ./engines/Llama-2-7b-chat-hf-trtllm-more-outputs \
    --tokenizer_dir meta-llama/Llama-2-7b-chat-hf \
    --max_output_len 10 \
    --input_text "Hey, are you conscious?" \
    --use_py_session \
    --debug_mode
```

The stepwise input and output tensors will be written in each or the file named `./artifacts/ditto-run/step{i}.txt` or `./artifacts/trtllm-run/step{i}.txt` and  where `i` is the step index.

If you have added extra output tensors, the extra tensors will be saved as well.


## Comparing Tensors
```
ditto compare ./artifacts/ditto-run/step0.pt ./artifacts/trtllm-run/step0.pt
```
